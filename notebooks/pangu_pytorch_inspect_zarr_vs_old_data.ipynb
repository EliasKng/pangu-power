{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), os.pardir)))\n",
    "import importlib\n",
    "import wind_fusion.pangu_pytorch as pg\n",
    "\n",
    "importlib.reload(pg)\n",
    "\n",
    "# Example usage of the inference function\n",
    "result = pg.inference()\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare original pangu_pytorch input data with zarr data from lsdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "\n",
    "\n",
    "\n",
    "old_data_surface = xr.open_dataset(\"/hkfs/home/project/hk-project-test-mlperf/om1434/masterarbeit/wind_fusion/pangu_pytorch/data/surface/surface_201907.nc\")\n",
    "old_data_surface = old_data_surface.sel(time=\"2019-07-01 00:00:00\")\n",
    "\n",
    "old_data_upper = xr.open_dataset(\"/home/hk-project-test-mlperf/om1434/masterarbeit/wind_fusion/pangu_pytorch/data/upper/upper_20190701.nc\")\n",
    "old_data_upper = old_data_upper.sel(time=\"2019-07-01 00:00:00\")\n",
    "\n",
    "zarr_data = xr.open_dataset(\"/lsdf/kit/imk-tro/projects/Gruppe_Quinting/ec.era5/1959-2023_01_10-wb13-6h-1440x721.zarr\", engine='zarr')\n",
    "variable_mapping = {\n",
    "    \"geopotential\": \"z\",\n",
    "    \"specific_humidity\": \"q\",\n",
    "    \"temperature\": \"t\",\n",
    "    \"u_component_of_wind\": \"u\",\n",
    "    \"v_component_of_wind\": \"v\",\n",
    "    \"mean_sea_level_pressure\": \"msl\",\n",
    "    \"10m_u_component_of_wind\": \"u10\",\n",
    "    \"10m_v_component_of_wind\": \"v10\",\n",
    "    \"2m_temperature\": \"t2m\"\n",
    "}\n",
    "\n",
    "surface_variables = [\"msl\", \"u10\", \"v10\", \"t2m\"]\n",
    "upper_variables = [\"z\", \"q\", \"t\", \"u\", \"v\"]\n",
    "\n",
    "# Select and rename variables according to the mapping\n",
    "zarr_data = zarr_data[list(variable_mapping.keys())].rename(variable_mapping)\n",
    "\n",
    "input_upper_dataset = zarr_data[upper_variables]\n",
    "input_surface_dataset = zarr_data[surface_variables]\n",
    "\n",
    "input_surface_dataset = input_surface_dataset.sel(time=old_data_surface.time)\n",
    "input_upper_dataset = input_upper_dataset.sel(time=old_data_surface.time)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zarr_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_data_surface\n",
    "# old_data_upper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare the values of new vs old data (they should be the same)\n",
    "\n",
    "for var in old_data_surface.data_vars:\n",
    "    print(f\"Variable: {var}\")\n",
    "    print(f\"\\t99th Percentile: {((old_data_surface[var] - input_surface_dataset[var])/old_data_surface[var]).quantile(0.99).values}\")\n",
    "    print(f\"\\t1st Percentile: {((old_data_surface[var] - input_surface_dataset[var])/old_data_surface[var]).quantile(0.01).values}\")\n",
    "    print(f\"\\tMean: {((old_data_surface[var] - input_surface_dataset[var])/old_data_surface[var]).mean().values}\")\n",
    "    print(f\"\\tStandard Deviation: {((old_data_surface[var] - input_surface_dataset[var])/old_data_surface[var]).std().values}\")\n",
    "    print(f\"\\tMax: {((old_data_surface[var] - input_surface_dataset[var])/old_data_surface[var]).max().values}\")\n",
    "    print(f\"\\tMin: {((old_data_surface[var] - input_surface_dataset[var])/old_data_surface[var]).min().values}\")\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resume: The differences are very small. For t2m & msl theyre irrelevant. For v10 and u10, the 99th percentile of deviations is 0.2% of deviations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for var in old_data_upper.data_vars:\n",
    "    print(f\"Variable: {var}\")\n",
    "    print(f\"\\t99th Percentile: {((old_data_upper[var] - input_upper_dataset[var])/old_data_upper[var]).quantile(0.99).values}\")\n",
    "    print(f\"\\t1st Percentile: {((old_data_upper[var] - input_upper_dataset[var])/old_data_upper[var]).quantile(0.01).values}\")\n",
    "    print(f\"\\tMean: {((old_data_upper[var] - input_upper_dataset[var])/old_data_upper[var]).mean().values}\")\n",
    "    print(f\"\\tStandard Deviation: {((old_data_upper[var] - input_upper_dataset[var])/old_data_upper[var]).std().values}\")\n",
    "    print(f\"\\tMax: {((old_data_upper[var] - input_upper_dataset[var])/old_data_upper[var]).max().values}\")\n",
    "    print(f\"\\tMin: {((old_data_upper[var] - input_upper_dataset[var])/old_data_upper[var]).min().values}\")\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resume for upper variables: All deviations seem reasonably small, except for q (specific humidity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print pairwise values of old_data_upper['q'] and input_upper_dataset['q']\n",
    "old_q_values = old_data_upper['q'].values\n",
    "input_q_values = input_upper_dataset['q'].values\n",
    "\n",
    "for old_val, input_val in zip(old_q_values.flatten(), input_q_values.flatten()):\n",
    "    print(f\"Old q: {old_val}, Input q: {input_val}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After inspecting q (specific humidity):\n",
    "- The two datasets are not the same.\n",
    "- However they are still \"similar\": They stay in the same range\n",
    "- Probably the differences are due to the numbers being super small (e-6), and old_data uses double precision while zarr data is single precision"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
